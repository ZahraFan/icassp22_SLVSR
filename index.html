<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0053)https://www.icst.pku.edu.cn/struct/Projects/SABA.html -->
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="language" content="english">
<title>SLVSR</title>
<meta name="description" content="Online Action Detection">
<meta name="author" content="Yanghao Li">
<link rel="icon" type="image/x-icon" href="http://www.icst.pku.edu.cn/favicon.ico">
<link rel="stylesheet" type="text/css" href="./support/project.css">
</head>

<body>
<div id="main">
  
	<div class="content"><br>
		<!-- <h1>SELF-LEARNED VIDEO SUPER-RESOLUTION WITH AUGMENTED SPATIAL AND TEMPORAL CONTEXT</h1> -->
		<h1>Self-Learned Video Super-Resolution With Augmented Spacial and Temporal Context Supplementary Material</h1>
		<div class="authors">
<!--         	<div class="author">
				 <a href="" style="text-decoration: none">&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp</a>
			</div>
			<div class="author">
				 <a href="https://huyuzhang.github.io/" style="text-decoration: none">Yuzhang Hu</a>
			</div>
			<div class="author">
				 <a href="http://www.icst.pku.edu.cn/struct/people/xsf.html" style="text-decoration: none">Sifeng Xia</a>
			</div>        
			<div class="author">
				 <a href="https://flyywh.github.io/" style="text-decoration: none">Wenhan Yang</a>
		    </div>
			<div class="author">
				 <a href="http://39.96.165.147/people/liujiaying.html" style="text-decoration: none">Jiaying Liu</a>
			</div> -->


		</div>
		<br>
		<br>
		<div class="overview sec">
	  	<!-- <p class="banner" align="center"><em>IEEE International Symposium on Circuits and Systems (ISCAS), 2020. </em></p> -->
		<!-- <div class="overview sec"> -->
			<!-- <div class="picture_wrapper"> -->
<!-- 				<div align="center">
		  			<img align="center" src="./support/teasor.png" width="45%" > -->
		  		<!-- <p style="text-align: left">Fig.1 Architecture of the Memory-Augmented Auto-Regressive Network (MAAR-Net). The network uses an auto-encoder as the feature extraction module -->
					<!-- based on previous frames. The auto-regressive coefficients are generated by short-term temporal redundancy and long-term temporal dynamics. The quality -->
					<!-- attention is injected for better prediction. A ConvLSTM-based memory update module is proposed to guide the coefficient generation to make full use of the -->
					<!-- information of all previous frames during the whole coding process.</p> -->
				<!-- </div> -->
	  		<!-- </div> -->
	  		<div align="center">
		  		<img align="center" src="./support/teasor.png" width="55%" > 
		  	</div>
	  	</div>

		<div class="Abstract sec">
			<h2>Abstract</h2>
			<div class="desp">

				<p style="text-align:justify">
					Video super-resolution methods typically rely on paired training data, in which the low-resolution frames are usually synthetically generated under predetermined degradation conditions (e.g., Bicubic downsampling).

					However, in real applications, it is labor-consuming and expensive to obtain this kind of training data, which limits the practical performance of these methods.

					To address the issue and get rid of the synthetic paired data, in this paper, we make exploration in utilizing the internal self-similarity redundancy within the video to build a Self-Learned Video Super-Resolution (SLVSR) method, which only needs to be trained on the input testing video itself.

					We employ a series of data augmentation strategies to make full use of the spatial and temporal context of the target video clips.

					The idea is applied to two branches of mainstream SR methods: frame fusion and frame recurrence methods. 

					Since the former takes advantage of the short-term temporal consistency and the latter of the long-term one, our method can satisfy different practical situations.

					The experimental results show the superiority of our proposed method, especially in addressing the video super-resolution problems in real applications.

				</p>

			</div>
			<!-- <div align="center" id="flowchart">
		  		<img src='SABA/figures/Flowchart.png' width='60%' >
		  		<p style="text-align: left">Fig. 2. Architecture of the proposed joint classification-regression RNN framework for online action detection and forecasting.</p>
	  		</div> -->
		</div>
		<br>


		<div class="download sec">
			<h2>Experimental Results</h2>
			<p style="text-align:justify">
				Note: We tuned the hyper-parameters of our method and improved the performance significantly compared with the results described in the paper. We will update the results if the paper is accepted. 
			</p>

			<h3>Quantitative Comparison for Blind Case</h3>
			<div align="center">
				<img align="center" src="./support/table_blind.png" width="60%">
				<!-- <p style="text-align: center">Table 1. Results of rate reduction of the proposed method. </p> -->
				<!-- <br>
				<img align="center" src="./support/vis.png" width="100%">
				<p style="text-align: center">Fig 2. Visual comparison of frames: (a) The original frame; (b) The reconstructed frame of the HEVC anchor; (c) The reconstructed frame of our method
					with the artificial reference frame.</p> -->
			</div>
			<br>
			<h3>Visualization Comparison for Blind Case</h3>
			<div align="center">
				<img align="center" src="./support/blind_fig1.png" width="100%">
			</div>
			<br>
			<div align="center">
				<img align="center" src="./support/foliage_blind_comparison.png" width="100%">
			</div>
			<br>

			<h3>Visualization Comparison for Real-World Video</h3>
		</div>
		<br>
		<br>


  </div>
</div>


</body></html>